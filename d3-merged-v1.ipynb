{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ba960-7ae0-4351-9d62-87914604ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports libraries for tiff loading, analysis, and plotting\n",
    "import tifffile as tiff\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter, find_peaks\n",
    "from scipy.ndimage import label\n",
    "from skimage.measure import block_reduce\n",
    "from collections import defaultdict\n",
    "plt.style.use('Jlab2.mplstyle') #for stylistic edits of graphs etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f209150-cbfa-467d-9293-dacb8d729c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    " Loads the raw tiff file as a stacked numpy array (frames x width in pixels x height in pixels)\n",
    "# Could be something that could be easily automated to go through all files in the folder\n",
    "tif_stack = tiff.imread(r\"C:\\\\Users\\\\teaching\\\\OneDrive - Nexus365\\\\ILESLA Y1 MODULES\\\\LSS STATS, DATA SCI & AI\\\\ILESLA\\\\Raw Data\\\\20251118_GRAB_ACh_WT_Male\\\\Slice 2 (DLS)\\\\Control\\\\_10\\\\_10_MMStack_Default.ome.tif\")\n",
    "\n",
    "\n",
    "# Loads the mask (a binary image) and convert it to a boolean array\n",
    "# Essentially a crop, to get FIJI to focus on important bits\n",
    "mask = tiff.imread(r\"C:\\Users\\teaching\\OneDrive - Nexus365\\ILESLA Y1 MODULES\\LSS STATS, DATA SCI & AI\\ILESLA\\Raw Data\\20251118_GRAB_ACh_WT_Male\\DLS Mask.tif\").astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169df583-d294-4fe2-b44a-4efe3dc216de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the number of spatial bins to segment x and y axes\n",
    "# Could experiment with a different number of bins? \n",
    "bin_no = 30\n",
    "\n",
    "# Applies the mask to set all masked pixels to NaN (non-value)\n",
    "tif_stack_masked = np.where(mask, np.nan, tif_stack)\n",
    "\n",
    "#  Defines bins along Y and X\n",
    "num_bins_y, num_bins_x = bin_no, bin_no\n",
    "\n",
    "# Computes bin size (pixels per bin in Y & X)\n",
    "bin_size_y, bin_size_x = tif_stack_masked.shape[1] // num_bins_y, tif_stack_masked.shape[2] // num_bins_x\n",
    "\n",
    "# Computes cropped height and width\n",
    "cropped_height, cropped_width = bin_size_y * num_bins_y, bin_size_x * num_bins_x\n",
    "\n",
    "# Crops the stack to divisible dimensions\n",
    "cropped_stack = tif_stack_masked[:, :cropped_height, :cropped_width]\n",
    "\n",
    "# Bins the stack using block averaging (ie averages all pixel values in 1 bin)\n",
    "binned_stack = block_reduce(\n",
    "    cropped_stack, \n",
    "    block_size=(1, bin_size_y, bin_size_x), \n",
    "    func=np.nanmean)\n",
    "\n",
    "# Identifies masked bins\n",
    "binned_mask = block_reduce(\n",
    "    mask[:cropped_height, :cropped_width], \n",
    "    block_size=(bin_size_y, bin_size_x), \n",
    "    func=np.nanmax)\n",
    "\n",
    "# Sets masked bins to NaN\n",
    "binned_stack[:, binned_mask == 1] = np.nan\n",
    "\n",
    "# Computes the minimum nonNaN bin value\n",
    "# Sets the background 0 value\n",
    "min_bin_value = np.nanmin(np.mean(binned_stack, axis=0))\n",
    "\n",
    "# Replaces NaN values with the minimum value\n",
    "# IE pretends that NaN values = lowest min background noise\n",
    "binned_stack = np.nan_to_num(binned_stack, nan=min_bin_value)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(10, 10))\n",
    "# Plots the average projection of the raw tiff file across the full recording\n",
    "ax[0].imshow(np.mean(tif_stack, axis=0), cmap='gray')\n",
    "ax[0].set(title='Avg. Raw Image')\n",
    "ax[0].axis('off')\n",
    "\n",
    "# Plots the mask to show the area(s) excluded from subsequent analyses\n",
    "ax[1].imshow(mask, cmap='gray')\n",
    "ax[1].set(title='Mask')\n",
    "ax[1].axis('off')\n",
    "\n",
    "# Plots mean activity across the binned FOV (Field of View)\n",
    "img = ax[2].imshow(np.mean(binned_stack, axis=0), cmap='viridis', aspect='auto')\n",
    "ax[2].set_title('Binned FOV')\n",
    "ax[2].set(xlabel='Bin', ylabel='Bin')\n",
    "ax[2].set_aspect('equal')\n",
    "ax[2].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2fd8ed-8b3d-4ab8-b161-e9458ae1124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excludes the first n frames of data (i.e. exponential decay artefact) \n",
    "excluded_frames = 50\n",
    "trunc_stack = binned_stack[excluded_frames:]\n",
    "\n",
    "fig, ax = plt.subplots(2,1, figsize=(3,3.5))\n",
    "ax[0].plot(binned_stack.mean(axis=(1,2)), linewidth=2)\n",
    "ax[0].axvspan(excluded_frames, len(tif_stack), color='C1', alpha=0.5)\n",
    "ax[0].get_yaxis().set_visible(False)\n",
    "ax[0].set(title='Avg. Trace')\n",
    "\n",
    "ax[1].plot(trunc_stack.mean(axis=(1,2)), color='C1', linewidth=2)\n",
    "ax[1].get_yaxis().set_visible(False)\n",
    "ax[1].set(title='Avg. Truncated', xlabel='Frames')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7b47a-8d02-49df-bc01-3014a4c70265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_dff(trunc_stack, window_size=100, percentile=8):\n",
    "    \"\"\"\n",
    "    Calculate ΔF/F using a sliding window with a specified percentile.\n",
    "\n",
    "    Parameters:\n",
    "        trunc_stack (np.ndarray): 3D array of fluorescence data (frames, height, width).\n",
    "        window_size (int): Number of frames in the sliding window (ensure longer than events).\n",
    "        percentile (float): Percentile to calculate F₀ in each window.\n",
    "    \n",
    "    Returns:\n",
    "        dff_stack (np.ndarray): ΔF/F normalized fluorescence data for each bin.\n",
    "    \"\"\"\n",
    "    # Initialises F0 array\n",
    "    F0_sliding = np.empty_like(trunc_stack)\n",
    "\n",
    "    # Iterates over frames\n",
    "    for frame in range(trunc_stack.shape[0]):\n",
    "        # Defines the start and end of the window\n",
    "        start_frame = max(0, frame - window_size)\n",
    "        end_frame = min(trunc_stack.shape[0], frame + window_size)\n",
    "\n",
    "        # Calculates the percentile for F0 (ignoring NaNs)\n",
    "        F0_sliding[frame] = np.nanpercentile(\n",
    "            trunc_stack[max(0, frame - window_size):max(1, frame)], percentile, axis=0\n",
    "        )\n",
    "\n",
    "    # Calculates ΔF/F\n",
    "    dff_stack = (trunc_stack - F0_sliding) / F0_sliding\n",
    "    \n",
    "    return dff_stack\n",
    "\n",
    "dff_stack = sliding_dff(trunc_stack)\n",
    "\n",
    "# Gets the dimensions of the dff stack (frames, height, width)\n",
    "_, binned_height, binned_width = dff_stack.shape\n",
    "\n",
    "# Generates a list of all bin coordinates\n",
    "bins = [(y, x) for y in range(binned_height) for x in range(binned_width)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c2297f-50f5-4aa2-a802-b55e47b42927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_events(dff_stack, bins, filter_window=5, prominence=5, min_event_length=5, \n",
    "                  filter_polyorder=2, threshold=5, min_distance=2, frame_rate=10, percentile=90):\n",
    "\n",
    "    aligned_events = []    # Creates a list to store merged (aligned) events\n",
    "    all_event_frames = []  # Creates a list to store the frames of detected events\n",
    "\n",
    "    for bin_coords in bins: # Scans through each bin in frame\n",
    "        y, x = bin_coords\n",
    "        trace = dff_stack[:, y, x]  # Extracts ΔF/F trace for this specific bin\n",
    "        \n",
    "        # Smooths the trace using a Savitzky-Golay filter\n",
    "        smoothed_trace = savgol_filter(trace, window_length=int(filter_window), polyorder=filter_polyorder)\n",
    "        \n",
    "        # Computes percentile threshold and noise estimate (sigma via MAD)\n",
    "        # Defines at which point a signal is classed as an event??\n",
    "        percentile_threshold = np.nanpercentile(smoothed_trace, percentile)\n",
    "        mad = np.nanmedian(np.abs(smoothed_trace - np.nanmedian(smoothed_trace))) + 1e-12\n",
    "        sigma = 1.4826 * mad # A statistical trick to make MAD an unbiased estimator of stdev\n",
    "        \n",
    "        #Checks for understanding Sigma and MAD:\n",
    "        #print(\"Sigma is:\", sigma)\n",
    "        #print(\"MAD is:\", mad)\n",
    "        \n",
    "        # Detects peaks above threshold with required prominence and spacing\n",
    "        peaks, _ = find_peaks(smoothed_trace, \n",
    "                              height=threshold * sigma,\n",
    "                              prominence=prominence * sigma,\n",
    "                              distance=min_distance)\n",
    "\n",
    "        # Identifies event start/end around each peak\n",
    "        for peak in peaks:\n",
    "            start = peak                            # Search backward to event onset\n",
    "            while start > 0 and smoothed_trace[start - 1] > percentile_threshold:\n",
    "                start -= 1\n",
    "            end = peak                              # Search forward to even offset\n",
    "            while end < len(smoothed_trace) - 1 and smoothed_trace[end + 1] > percentile_threshold:\n",
    "                end += 1\n",
    "            if (end - start) >= min_event_length:   # Store only sufficiently long events\n",
    "                all_event_frames.append((peak, start, end, bin_coords))\n",
    "\n",
    "    # Sort all detected events by peak frame\n",
    "    all_event_frames.sort(key=lambda x: x[0])  \n",
    "    return all_event_frames\n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "    \n",
    "    aligned_events = []  # Creates a list to store merged (aligned) events\n",
    "    \n",
    "    for peak_frame, start, end, bin_coords in all_event_frames:\n",
    "        # Stores the details of the first event\n",
    "        if not aligned_events:\n",
    "            aligned_events.append({\n",
    "                'start': int(start),\n",
    "                'end': int(end),\n",
    "                'bins': [bin_coords],\n",
    "                'peak': int(peak_frame)  \n",
    "            })\n",
    "            continue\n",
    "\n",
    "        last = aligned_events[-1]   # Most recent grouped event\n",
    "        frame_range = 10            # Merge events whose peaks occur within ±10 frames\n",
    "        if abs(peak_frame - last['peak']) <= frame_range:\n",
    "            last['start'] = int(min(last['start'], start))   # Extend event start if needed\n",
    "            last['end']   = int(max(last['end'], end))       # Extend event end if needed\n",
    "            last['bins'].append(bin_coords)                  # Add contributing bin\n",
    "        else:\n",
    "            # Stores a new event\n",
    "            aligned_events.append({\n",
    "                'start': int(start),\n",
    "                'end': int(end),\n",
    "                'bins': [bin_coords],\n",
    "                'peak': int(peak_frame)\n",
    "            })\n",
    "    print(aligned_events)\n",
    "    print(len(aligned_events))\n",
    "    return all_event_frames\n",
    "\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c32a2e-79d4-4927-b610-738059fdb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_event_frames = detect_events(dff_stack, bins, filter_window=5, min_event_length=1,                    #Should this be greater???\n",
    "                               prominence=6, threshold=5, min_distance=1, percentile=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13edb51-63d3-4e53-a68c-087972bac557",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### LAILA V2 #######################\n",
    "\n",
    "# Input:\n",
    "#   aligned_events = list of { 'start':..., 'end':..., 'bins': [(y,x), ...], 'peak':... }\n",
    "# Output:\n",
    "#   distinct_event_coords = { \"event 1\": [(y,x),...], \"event 2\": [...] }\n",
    "\n",
    "# 1. PREPARE UNIQUE COORDINATE LISTS FOR LOOKUP\n",
    "all_coords = []                              # Makes a list of all bins containing fluorescence\n",
    "for ev in all_event_frames:                  # Adds all the event coordinates from the above bit to one easy to search list\n",
    "    all_coords.extend(ev['bins'])      \n",
    "coords_set = set(all_coords)                 # Makes the list a set for easier searching\n",
    "coords_list = sorted(coords_set)             # For deterministic processing order, optional but helpful for understanding\n",
    "\n",
    "# 2. SETUP DATA STRUCTURES\n",
    "distinct_event_coords = {}                   # Maps event name to list of (y,x) bin locations\n",
    "coord_to_event = {}                          # Maps (y,x) to the event name for adding extra bins to events in distinct_event_coords\n",
    "next_event_id = 1\n",
    "\n",
    "##MOVE TO NEXT CELL FROM HERE TO RUN SANJNA'S CODE\n",
    "\n",
    "# # helper: parse numeric suffix from \"event N\" for deterministic merging choice\n",
    "# def event_id_num(evname):\n",
    "#     try:\n",
    "#         return int(evname.split()[-1])\n",
    "#     except Exception:\n",
    "#         return float('inf')\n",
    "\n",
    "# # 3. NEIGHBOUR OFFSETS FOR QUEEN ADJACENCY.... using Sanjna's updates\n",
    "# from itertools import product\n",
    "# neighbour_offsets = [(dy, dx) for dy, dx in product((-1,0,1), repeat=2) if (dy,dx) != (0,0)] \n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "# 4. PROCESS EACH BIN FOR CATEGORISATION INTO A DISTINCT EVENT\n",
    "for coord in coords_list:\n",
    "    if coord in coord_to_event:\n",
    "        continue                                                                # ie move on if already assigned\n",
    "    \n",
    "    # 4a. Find all existing events that this current coordinate touches and adds to the set that fluorescent neighbour (nb)\n",
    "\n",
    "    touched_events = set()\n",
    "    for dy, dx in neighbour_offsets:\n",
    "        nb = (coord[0] + dy, coord[1] + dx)\n",
    "        if nb in coord_to_event:\n",
    "            touched_events.add(coord_to_event[nb])\n",
    "\n",
    "    # 4b. Make a new event if the coord of interest has no neighbours that have been accounted for already\n",
    "\n",
    "    if len(touched_events) == 0:\n",
    "        name = f\"event {next_event_id}\"\n",
    "        distinct_event_coords[name] = [coord]\n",
    "        coord_to_event[coord] = name\n",
    "        next_event_id += 1\n",
    "\n",
    "    # 4c. Add this bin coordinate to the existing event list, if it has 1 existing queen neighbour\n",
    "\n",
    "    elif len(touched_events) == 1:\n",
    "        name = next(iter(touched_events))\n",
    "        distinct_event_coords[name].append(coord)\n",
    "        coord_to_event[coord] = name\n",
    "        \n",
    "    # 4d. Creates a merged list of coordinates for if touching multiple bins already accounted for\n",
    "      \n",
    "    else:\n",
    "        # touches multiple events -> MERGE them all (and include this coord)\n",
    "        # pick canonical name (smallest numeric id) to keep deterministic\n",
    "        merged_name = min(touched_events, key=event_id_num)\n",
    "\n",
    "        # collect coords from all touched events and remove those events\n",
    "        merged_coords = []\n",
    "        for ev in list(touched_events):\n",
    "            merged_coords.extend(distinct_event_coords.pop(ev, []))\n",
    "\n",
    "        # add the new coord\n",
    "        merged_coords.append(coord)\n",
    "\n",
    "        # store merged coords under merged_name and update mapping\n",
    "        distinct_event_coords[merged_name] = merged_coords\n",
    "        for c in merged_coords:\n",
    "            coord_to_event[c] = merged_name\n",
    "\n",
    "#for name in distinct_event_coords:\n",
    " #   distinct_event_coords[name].sort()\n",
    "\n",
    "print(\"Distinct Event List: \", distinct_event_coords)\n",
    "\n",
    "sorted_distinct_event_coords = dict(\n",
    "    sorted(\n",
    "        distinct_event_coords.items(),\n",
    "        key=lambda kv: int(kv[0].split()[-1])\n",
    "    )\n",
    ")\n",
    "\n",
    "print(sorted_distinct_event_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd963a1-43c4-44b7-aa3c-e3cfa9891448",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Need this class of UnionFind to UnionMerge groups of events as they grow - this is the more robust\n",
    "#Run this before running the merge\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self):\n",
    "        self.parent = {}\n",
    "        self.rank = {}\n",
    "\n",
    "    def make(self, x):\n",
    "        if x not in self.parent:\n",
    "            self.parent[x] = x\n",
    "            self.rank[x] = 0\n",
    "\n",
    "    def find(self, x):\n",
    "        # path compression\n",
    "        if self.parent[x] != x:\n",
    "            self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "\n",
    "    def union(self, a, b):\n",
    "        # union by rank\n",
    "        ra = self.find(a)\n",
    "        rb = self.find(b)\n",
    "        if ra == rb:\n",
    "            return ra\n",
    "        if self.rank[ra] < self.rank[rb]:\n",
    "            ra, rb = rb, ra\n",
    "        self.parent[rb] = ra\n",
    "        if self.rank[ra] == self.rank[rb]:\n",
    "            self.rank[ra] += 1\n",
    "        return ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecfe420-8157-4535-8099-7581a9f70846",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Spatially separate the events now:\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "uf = UnionFind() #calls the class we defined above\n",
    "distinct_events = {}\n",
    "coord_to_group = {}\n",
    "counter = 1\n",
    "next_group_ID = 1\n",
    "\n",
    "for event in all_event_frames:\n",
    "    if event not in distinct_events: #we may want to remove this if statement and just loop already depending on how this runs\n",
    "        bin_y = all_event_frames[event][3][0]\n",
    "        bin_x = all_event_frames[event][3][1]\n",
    "        bin_coords = all_event_frames[event][3]\n",
    "        print(bin_coords) #just printing as a sanity check\n",
    "        coord = (y,x)\n",
    "        print(coord) #print this also a sa snity check - should match bin_coords above\n",
    "\n",
    "        #will need to modify/delete this in the final code where we add in the time component:\n",
    "        if coord in coord_to_group:\n",
    "            continue\n",
    "        \n",
    "        #building the catchment range - currently we have hard-coded a range of 8 Queen neighbours\n",
    "        catchment_range = [\n",
    "            (bin_y, bin_x+1), (bin_y, (bin_x-1), \n",
    "            (bin_y+1, bin_x+1), (bin_y+1, bin_x), \n",
    "            (bin_y-1, bin_x) , (bin_y-1, bin_x-1), \n",
    "            (bin_y-1, bin_x+1), (bin_y+1), (bin_x-1)\n",
    "            ]\n",
    "\n",
    "        # find groups present in neighbours (may be zero, one, or multiple)\n",
    "        neighbour_group_ids = {coord_to_group[n] for n in catchment if n in coord_to_group}\n",
    "        \n",
    "        #if any([b in distinct_events for b in catchment_range]):\n",
    "        if not neighbour_group_ids:\n",
    "            # no neighbour -> create new provisional group\n",
    "            gid = next_group_id\n",
    "            next_group_id += 1  \n",
    "            uf.make(gid)\n",
    "            coord_to_group[coord] = gid\n",
    "             \n",
    "        else:\n",
    "            # pick one gid as anchor (first in set) and attach coord to it\n",
    "            # then union all neighbour gids together so they become one set\n",
    "            # Note: converting set to list gives deterministic order in Python >=3.7,\n",
    "            # but we don't rely on order because union-find canonicalizes later.\n",
    "            gids = list(neighbour_group_ids)\n",
    "            anchor = gids[0]\n",
    "            coord_to_group[coord] = anchor\n",
    "            uf.make(anchor)  # in case it wasn't created (should be)\n",
    "            for other in gids[1:]:\n",
    "                uf.make(other)\n",
    "                uf.union(anchor, other)\n",
    "            # (anchor vs other order doesn't matter; uf keeps sets)\n",
    "\n",
    "# Second pass: canonicalize provisional gids -> root ids, relabel to contiguous 1..N\n",
    "# Build mapping provisional_gid -> root\n",
    "provgid_to_root = {}\n",
    "for gid in list(uf.parent.keys()):\n",
    "    provgid_to_root[gid] = uf.find(gid)\n",
    "\n",
    "# Map each root to a new compact id (1..M)\n",
    "root_to_newid = {}\n",
    "new_next = 1\n",
    "for root in sorted(set(provgid_to_root.values())):\n",
    "    root_to_newid[root] = new_next\n",
    "    new_next += 1\n",
    "\n",
    "# Build final coord -> group_id mapping\n",
    "coord_to_group = {}\n",
    "for coord, prov_gid in coord_to_provgid.items():\n",
    "    root = provgid_to_root[prov_gid]\n",
    "    final_gid = root_to_newid[root]\n",
    "    coord_to_group[coord] = final_gid\n",
    "\n",
    "# Build groups dict (final_gid -> list coords)\n",
    "groups = defaultdict(list)\n",
    "for coord, gid in coord_to_group.items():\n",
    "    groups[gid].append(coord)\n",
    "\n",
    "groups = {gid: sorted(coords) for gid, coords in groups.items()}\n",
    "\n",
    "# Build member_assignments aligned with all_event_frames\n",
    "member_assignments = []\n",
    "for ev in all_event_frames:\n",
    "    try:\n",
    "        _, _, _, bin_coords = ev\n",
    "    except Exception:\n",
    "        bin_coords = None\n",
    "\n",
    "    if bin_coords is None:\n",
    "        member_assignments.append(None)\n",
    "        continue\n",
    "\n",
    "    coord = (int(bin_coords[0]), int(bin_coords[1]))\n",
    "    member_assignments.append(coord_to_group.get(coord, None))\n",
    "\n",
    "return groups, member_assignments, coord_to_group"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
